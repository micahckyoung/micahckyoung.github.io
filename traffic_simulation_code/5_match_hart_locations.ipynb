{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "the purpose of this code is to match HART stations to the OD Matrix\n",
    "- then I can identify routes that would use HART stations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "this file was originally called 250414_match_hart_v1.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import geopandas as gpd\n",
    "import os\n",
    "\n",
    "pd.set_option('display.max_columns', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_dir = os.path.dirname(os.path.dirname(os.getcwd()))\n",
    "datasets_dir = os.path.join(base_dir, 'datasets')\n",
    "\n",
    "# Updated file paths\n",
    "tract_shape_path = os.path.join(datasets_dir, 'tl_2023_15_tract.zip')\n",
    "tract_gdf = gpd.read_file(tract_shape_path)\n",
    "\n",
    "station_path = os.path.join(datasets_dir, 'HART_Transit_Stations_PUBLIC_-7935673169750357749.zip')\n",
    "station_gdf = gpd.read_file(station_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sjoin the stations to tracts\n",
    "def merge_stations_and_tracts(station_gdf, tract_gdf):\n",
    "    if station_gdf.crs != tract_gdf.crs:\n",
    "        station_gdf = station_gdf.to_crs(tract_gdf.crs)\n",
    "    joined_gdf = gpd.sjoin(station_gdf, tract_gdf, how=\"left\", predicate=\"within\")\n",
    "    # Clean up column names from the join operation\n",
    "    if 'index_right' in joined_gdf.columns:\n",
    "        joined_gdf = joined_gdf.drop(columns=['index_right'])\n",
    "    return joined_gdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of stations: 21\n",
      "Number of tracts covered: 18\n"
     ]
    }
   ],
   "source": [
    "stations_with_tracts = merge_stations_and_tracts(station_gdf, tract_gdf)\n",
    "print(f\"Number of stations: {len(stations_with_tracts)}\")\n",
    "print(f\"Number of tracts covered: {len(stations_with_tracts['GEOID'].unique())}\")\n",
    "station_tract_list = stations_with_tracts['GEOID'].unique().tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "read od matrix and filter for rows that end in the station tracts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_dir = os.path.dirname(os.path.dirname(os.getcwd()))\n",
    "processed_dir = os.path.join(base_dir, 'datasets', 'processed')\n",
    "od_path = os.path.join(processed_dir, 'od_matrix.csv')\n",
    "\n",
    "od_df = pd.read_csv(od_path)\n",
    "od_df['origin_tract'] = od_df['h_geocode'].astype('str').str[:11]\n",
    "od_df['destination_tract'] = od_df['w_geocode'].astype('str').str[:11]\n",
    "\n",
    "od_df_filtered = od_df[od_df['destination_tract'].isin(station_tract_list)].copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "there are ~52k vehicles that end in the station tracts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "find every road link for each vehicle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "import json\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_edges_path():\n",
    "    edges_path = os.path.join(os.path.dirname(os.getcwd()), 'datasets', 'processed', 'edges.gpkg')\n",
    "    return edges_path\n",
    "\n",
    "def read_edges(edges_path):\n",
    "    edges = gpd.read_file(edges_path)\n",
    "    return edges\n",
    "\n",
    "def create_network_graph(edges_df):\n",
    "    \"\"\"Create directed graph from edges dataframe using free_flow_travel_time as weight\"\"\"\n",
    "    G = nx.DiGraph()\n",
    "    \n",
    "    # Add edges with their attributes\n",
    "    for _, row in edges_df.iterrows():\n",
    "        G.add_edge(\n",
    "            str(row['Node1']),\n",
    "            str(row['Node2']),\n",
    "            weight=row['free_flow_travel_time'],  # Using free flow travel time as weight\n",
    "            length=row['length'],\n",
    "            length_miles=row['length_miles'] if 'length_miles' in row else None,\n",
    "            FFS=row['FFS'] if 'FFS' in row else None, \n",
    "            link_id=row['link_id']\n",
    "        )\n",
    "    \n",
    "    print(f\"Network created with {G.number_of_nodes()} nodes and {G.number_of_edges()} edges\")\n",
    "    return G\n",
    "\n",
    "def convert_node_paths_to_links(vehicles_df, edges_df):\n",
    "    \"\"\"\n",
    "    Convert node paths to link sequences using the edges dataframe.\n",
    "    Returns the vehicles_df with an additional 'route_links' column.\n",
    "    \"\"\"\n",
    "    # Create node pair to link_id lookup\n",
    "    link_lookup = {(str(row['Node1']), str(row['Node2'])): row['link_id'] \n",
    "                  for _, row in edges_df.iterrows()}\n",
    "    \n",
    "    # Initialize route_links column\n",
    "    vehicles_df['route_links'] = None\n",
    "    vehicles_df['conversion_status'] = 'pending'\n",
    "    \n",
    "    # Process only vehicles with successful node paths\n",
    "    mask = vehicles_df['routing_status'] == 'success'\n",
    "    print(f\"Converting node paths to links for {mask.sum()} vehicles...\")\n",
    "    \n",
    "    success_count = 0\n",
    "    fail_count = 0\n",
    "    \n",
    "    for idx, row in vehicles_df[mask].iterrows():\n",
    "        try:\n",
    "            node_path = json.loads(row['node_path'])\n",
    "            path_links = []\n",
    "            path_is_continuous = True\n",
    "            \n",
    "            for i in range(len(node_path) - 1):\n",
    "                node_pair = (str(node_path[i]), str(node_path[i + 1]))\n",
    "                if node_pair not in link_lookup:\n",
    "                    path_is_continuous = False\n",
    "                    break\n",
    "                path_links.append(link_lookup[node_pair])\n",
    "            \n",
    "            if path_is_continuous and path_links:\n",
    "                vehicles_df.loc[idx, 'route_links'] = json.dumps(path_links)\n",
    "                vehicles_df.loc[idx, 'conversion_status'] = 'success'\n",
    "                success_count += 1\n",
    "            else:\n",
    "                vehicles_df.loc[idx, 'conversion_status'] = 'discontinuous'\n",
    "                fail_count += 1\n",
    "                \n",
    "        except Exception as e:\n",
    "            vehicles_df.loc[idx, 'conversion_status'] = f'error: {str(e)}'\n",
    "            fail_count += 1\n",
    "            \n",
    "        # Update progress\n",
    "        if (success_count + fail_count) % 1000 == 0:\n",
    "            print(f\"Processed {success_count + fail_count} vehicles...\")\n",
    "    \n",
    "    print(f\"\\nLink Conversion Summary:\")\n",
    "    print(f\"Successful conversions: {success_count}\")\n",
    "    print(f\"Failed conversions: {fail_count}\")\n",
    "    \n",
    "    return vehicles_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_node_paths(G, od_matrix):\n",
    "    \"\"\"\n",
    "    Calculate the node-to-node paths for each origin-destination pair.\n",
    "    Returns the od_matrix with an additional 'node_path' column.\n",
    "    \"\"\"\n",
    "    # Initialize output dataframe\n",
    "    vehicles_df = od_matrix.copy()\n",
    "    vehicles_df['node_path'] = None\n",
    "    vehicles_df['routing_status'] = 'pending'\n",
    "    \n",
    "    # Calculate paths for unique OD pairs\n",
    "    unique_od_pairs = od_matrix.groupby(['origin_node', 'destination_node']).size().reset_index()\n",
    "    print(f\"Calculating paths for {len(unique_od_pairs)} unique OD pairs...\")\n",
    "    \n",
    "    # Create a progress counter\n",
    "    processed = 0\n",
    "    for _, row in unique_od_pairs.iterrows():\n",
    "        origin = str(row['origin_node'])\n",
    "        destination = str(row['destination_node'])\n",
    "        \n",
    "        try:\n",
    "            # Find shortest path\n",
    "            path_nodes = nx.shortest_path(G, origin, destination, weight='weight')\n",
    "            \n",
    "            # Update all vehicles with this OD pair\n",
    "            mask = ((vehicles_df['origin_node'] == row['origin_node']) & \n",
    "                   (vehicles_df['destination_node'] == row['destination_node']))\n",
    "            vehicles_df.loc[mask, 'node_path'] = json.dumps(path_nodes)  # Store as JSON string temporarily\n",
    "            vehicles_df.loc[mask, 'routing_status'] = 'success'\n",
    "            \n",
    "        except nx.NetworkXNoPath:\n",
    "            mask = ((vehicles_df['origin_node'] == row['origin_node']) & \n",
    "                   (vehicles_df['destination_node'] == row['destination_node']))\n",
    "            vehicles_df.loc[mask, 'routing_status'] = 'no_path'\n",
    "        \n",
    "        # Update progress\n",
    "        processed += 1\n",
    "        if processed % 100 == 0:\n",
    "            print(f\"Processed {processed} of {len(unique_od_pairs)} OD pairs...\")\n",
    "    \n",
    "    # Print summary\n",
    "    success_count = (vehicles_df['routing_status'] == 'success').sum()\n",
    "    fail_count = (vehicles_df['routing_status'] != 'success').sum()\n",
    "    print(f\"\\nPath Calculation Summary:\")\n",
    "    print(f\"Successful paths: {success_count}\")\n",
    "    print(f\"Failed paths: {fail_count}\")\n",
    "    \n",
    "    return vehicles_df\n",
    "\n",
    "def create_route_assignment_data(od_matrix):\n",
    "    # Read input data\n",
    "    print(\"Reading input data...\")\n",
    "    \n",
    "    edges_path = read_edges_path()\n",
    "    edges_df = read_edges(edges_path)\n",
    "    \n",
    "    # Make sure edges have the right datatypes\n",
    "    edges_df['link_id'] = edges_df['link_id'].astype(str)\n",
    "    edges_df['Node1'] = edges_df['Node1'].astype(str)\n",
    "    edges_df['Node2'] = edges_df['Node2'].astype(str)\n",
    "    \n",
    "    # Ensure free_flow_travel_time exists in seconds\n",
    "    if 'free_flow_travel_time' not in edges_df.columns:\n",
    "        if 'FFS' in edges_df.columns and 'length_miles' in edges_df.columns:\n",
    "            # Calculate free flow travel time in seconds\n",
    "            # FFS is in mph, length_miles is in miles\n",
    "            # Time = distance / speed, converted to seconds\n",
    "            edges_df['free_flow_travel_time'] = (edges_df['length_miles'] / edges_df['FFS']) * 3600\n",
    "        else:\n",
    "            raise ValueError(\"Cannot calculate free_flow_travel_time - missing FFS or length_miles columns\")\n",
    "    \n",
    "    # Create network graph\n",
    "    G = create_network_graph(edges_df)\n",
    "    \n",
    "    # Calculate optimal routes\n",
    "    vehicles_with_paths = calculate_node_paths(G, od_matrix)\n",
    "    \n",
    "    # Convert node paths to link sequences\n",
    "    vehicles_with_links = convert_node_paths_to_links(vehicles_with_paths, edges_df)\n",
    "\n",
    "    # Convert the DataFrame to a list of dictionaries, preserving the actual lists\n",
    "    # instead of JSON strings in the node_path and route_links columns\n",
    "    result = []\n",
    "    for _, row in vehicles_with_links.iterrows():\n",
    "        vehicle_dict = row.to_dict()\n",
    "        \n",
    "        # Parse JSON string back to list if not None\n",
    "        if vehicle_dict.get('node_path'):\n",
    "            vehicle_dict['node_path'] = json.loads(vehicle_dict['node_path'])\n",
    "        \n",
    "        if vehicle_dict.get('route_links'):\n",
    "            vehicle_dict['route_links'] = json.loads(vehicle_dict['route_links'])\n",
    "        \n",
    "        result.append(vehicle_dict)\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "find the earliest node in the list that falls within one of the highway tracts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nodes_path = os.path.join(processed_dir, 'nodes.gpkg')\n",
    "nodes_gdf = gpd.read_file(nodes_path)\n",
    "\n",
    "# find tract for these nodes\n",
    "nodes_tracts = merge_stations_and_tracts(nodes_gdf, tract_gdf)[['Node', 'GEOID']]\n",
    "# filter for station tracts\n",
    "station_nodes_tract_level = nodes_tracts[nodes_tracts['GEOID'].isin(station_tract_list)]\n",
    "station_nodes_list = station_nodes_tract_level['Node'].unique().tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_station_node_indicators(vehicles_json, station_nodes_list):\n",
    "    # Convert station_nodes_list to string for consistent comparison\n",
    "    station_nodes_set = set(str(node) for node in station_nodes_list)\n",
    "    \n",
    "    for vehicle in vehicles_json:\n",
    "        # Skip vehicles with no path\n",
    "        if not vehicle.get('node_path'):\n",
    "            vehicle['station_node_indicators'] = []\n",
    "            continue\n",
    "            \n",
    "        # Create binary indicators\n",
    "        node_path = vehicle['node_path']\n",
    "        station_indicators = [1 if str(node) in station_nodes_set else 0 for node in node_path]\n",
    "        \n",
    "        # Add to vehicle dictionary\n",
    "        vehicle['station_node_indicators'] = station_indicators\n",
    "    \n",
    "    return vehicles_json\n",
    "\n",
    "def add_node_statistics(vehicles_json):\n",
    "    \"\"\"\n",
    "    Add statistics about nodes in the path:\n",
    "    1. total_nodes: Total number of nodes in the path\n",
    "    2. earliest_station_position: Index of first station node (or -1 if none found)\n",
    "    3. earliest_station_node: The first station node in the path (or None if none found)\n",
    "    \n",
    "    Parameters:\n",
    "    vehicles_json (list): List of vehicle dictionaries with node_path and station_node_indicators\n",
    "    \n",
    "    Returns:\n",
    "    list: Updated vehicles_json with additional statistics\n",
    "    \"\"\"\n",
    "    for vehicle in vehicles_json:\n",
    "        # Skip vehicles with no path\n",
    "        if not vehicle.get('node_path'):\n",
    "            vehicle['total_nodes'] = 0\n",
    "            vehicle['earliest_station_position'] = -1\n",
    "            vehicle['earliest_station_node'] = None\n",
    "            continue\n",
    "            \n",
    "        # Add total count of nodes\n",
    "        vehicle['total_nodes'] = len(vehicle['node_path'])\n",
    "        \n",
    "        # Find position of earliest station node (first 1 in the indicator list)\n",
    "        indicators = vehicle.get('station_node_indicators', [])\n",
    "        node_path = vehicle.get('node_path', [])\n",
    "        \n",
    "        try:\n",
    "            # Find the index of the first 1 (if any)\n",
    "            earliest_station = indicators.index(1)\n",
    "            vehicle['earliest_station_position'] = earliest_station\n",
    "            \n",
    "            # Get the actual node value at this position\n",
    "            vehicle['earliest_station_node'] = node_path[earliest_station]\n",
    "        except ValueError:\n",
    "            # If no 1 found in the list, set to -1 and None\n",
    "            vehicle['earliest_station_position'] = -1\n",
    "            vehicle['earliest_station_node'] = None\n",
    "    \n",
    "    return vehicles_json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# route_assignment_json = create_route_assignment_data(od_df_filtered)\n",
    "# route_assignment_json = add_station_node_indicators(route_assignment_json, station_nodes_list)\n",
    "# route_assignment_json = add_node_statistics(route_assignment_json)\n",
    "\n",
    "# save_path = os.path.join(base_dir, 'datasets')\n",
    "# output_path = os.path.join(save_path, \"250414_hart_mapping.json\")\n",
    "\n",
    "# def save_route_data_to_json(vehicles_json, output_path):\n",
    "#     # Ensure the directory exists\n",
    "#     os.makedirs(os.path.dirname(output_path), exist_ok=True)\n",
    "    \n",
    "#     # Save the JSON data with nice formatting\n",
    "#     with open(output_path, 'w') as json_file:\n",
    "#         json.dump(vehicles_json, json_file, indent=2)\n",
    "    \n",
    "#     print(f\"Data successfully saved to: {output_path}\")\n",
    "#     return output_path\n",
    "\n",
    "# save_route_data_to_json(route_assignment_json, output_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### read back the json and merge it into the od matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "read_path = os.path.join(processed_dir, '250414_hart_mapping.json')\n",
    "with open(read_path, 'r') as json_file:\n",
    "    route_assignment_json = json.load(json_file)\n",
    "# Convert to DataFrame for easier manipulation\n",
    "route_assignment_df = pd.json_normalize(route_assignment_json)[['person_id', 'total_nodes', 'earliest_station_position', 'earliest_station_node']].copy()\n",
    "\n",
    "# merge with od matrix\n",
    "route_assignment_df['person_id'] = route_assignment_df['person_id'].astype(str)\n",
    "od_df['person_id'] = od_df['person_id'].astype(str)\n",
    "merged_od_df = pd.merge(od_df, route_assignment_df, left_on='person_id', right_on='person_id', how='left')\n",
    "\n",
    "save_path = processed_dir\n",
    "# merged_od_df.to_csv(os.path.join(save_path, \"250414_od_matrix_with_hart_designation.csv\"), index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "250110_traffic_simulation",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
